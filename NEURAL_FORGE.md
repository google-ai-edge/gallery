# Neural Forge Architecture

Neural Forge is a comprehensive mobile AI platform built on the foundation of Google's AI Edge Gallery, extended with universal model format support, intelligent download management, and a vision for advanced on-device AI capabilities.

## ğŸ¯ Vision

Transform Android devices into portable AI powerhouses that can:
- Load and run models in multiple formats (TFLite, ONNX, PyTorch, LiteRT-LM)
- Download large models efficiently with resume capability
- Convert between model formats on-device
- Optimize models for mobile hardware
- Chain models together for complex workflows
- Share models peer-to-peer without internet
- Fine-tune models on-device

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Neural Forge Engine                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           Model Registry & Orchestration              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚              â”‚              â”‚              â”‚
         â–¼              â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Download    â”‚ â”‚   Format     â”‚ â”‚  Conversion  â”‚ â”‚  Execution   â”‚
â”‚  Manager     â”‚ â”‚  Detection   â”‚ â”‚  Pipeline    â”‚ â”‚  Engines     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                   â”‚              â”‚              â”‚
     â”‚                   â”‚              â”‚              â”œâ”€ ONNX Runtime
     â”‚                   â”‚              â”‚              â”œâ”€ TFLite
     â”‚                   â”‚              â”‚              â””â”€ LiteRT-LM
     â–¼                   â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Device Hardware                            â”‚
â”‚  CPU | GPU (Adreno) | NPU | DSP (Hexagon) | NNAPI           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Core Components

### 1. Neural Forge Engine (`NeuralForgeEngine.kt`)

The central orchestrator that manages all model operations.

**Responsibilities:**
- Model lifecycle management (load, unload, registry)
- Coordination between download, conversion, and execution
- Device capability detection
- Resource management

**Key APIs:**
```kotlin
// Download a model
val downloadFlow = engine.downloadModel(url, modelName)

// Load a model
val result = engine.loadModel(modelId, modelName, OptimizationPreset.BALANCED)

// Get device capabilities
val capabilities = engine.getDeviceCapabilities()
```

### 2. Model Download Manager (`ModelDownloadManager.kt`)

Enhanced download system with enterprise-grade features.

**Features:**
- âœ… Chunked downloads for large models
- âœ… Resume capability (HTTP Range requests)
- âœ… Automatic retry with exponential backoff
- âœ… Real-time progress tracking (MB/s, ETA)
- âœ… Split model support (merge multiple parts)
- âœ… Concurrent download management

**Architecture:**
```kotlin
ModelDownloadManager
â”œâ”€â”€ OkHttpClient (with custom interceptors)
â”‚   â”œâ”€â”€ ProgressInterceptor      // Track download progress
â”‚   â”œâ”€â”€ RetryInterceptor         // Auto-retry on failures
â”‚   â””â”€â”€ ConnectionPool           // Efficient connection reuse
â””â”€â”€ Download State Management
    â”œâ”€â”€ Preparing
    â”œâ”€â”€ Downloading(progress)
    â”œâ”€â”€ Merging (for split models)
    â”œâ”€â”€ Completed(file)
    â””â”€â”€ Failed(error)
```

**Usage Example:**
```kotlin
downloadManager.downloadModel(url, "model.onnx", enableResume = true)
    .collect { state ->
        when (state) {
            is DownloadState.Downloading -> {
                val progress = state.progress
                println("${progress.progressPercent}% @ ${progress.downloadRate} MB/s")
            }
            is DownloadState.Completed -> {
                println("Downloaded to: ${state.file}")
            }
        }
    }
```

### 3. Model Format Detection (`ModelFormat.kt`, `ModelFormatDetector.kt`)

Intelligent format detection system.

**Supported Formats:**
- **TensorFlow Lite** (.tflite) - Magic bytes: `TFL3`
- **ONNX** (.onnx) - Protocol Buffers format
- **PyTorch Mobile** (.pt, .pth) - ZIP-based format
- **LiteRT-LM** (.litertlm) - Google's optimized LLM format

**Detection Strategy:**
1. File extension check (fast path)
2. Magic byte analysis (fallback)
3. Format validation

```kotlin
val detector = ModelFormatDetector()
val format = detector.detectFormat(modelFile)

when (format) {
    ModelFormat.TensorFlowLite -> // Load with TFLite
    ModelFormat.ONNX -> // Load with ONNX Runtime
    ModelFormat.LiteRTLM -> // Load with LiteRT
}
```

### 4. ONNX Inference Engine (`ONNXInferenceEngine.kt`)

Full ONNX Runtime integration for Android.

**Features:**
- âœ… Multi-threaded CPU inference
- âœ… Graph optimization (all optimization levels)
- âœ… Session management
- âœ… Input/output introspection
- ğŸ”„ GPU acceleration (Phase 2)
- ğŸ”„ NNAPI delegation (Phase 2)

**Architecture:**
```kotlin
ONNXInferenceEngine
â”œâ”€â”€ OrtEnvironment (singleton)
â”œâ”€â”€ SessionOptions
â”‚   â”œâ”€â”€ Thread configuration
â”‚   â”œâ”€â”€ Optimization level
â”‚   â””â”€â”€ Execution providers
â””â”€â”€ Session Management
    â”œâ”€â”€ Load model
    â”œâ”€â”€ Run inference
    â”œâ”€â”€ Multi-input support
    â””â”€â”€ Resource cleanup
```

**Example:**
```kotlin
// Load ONNX model
val sessionResult = onnxEngine.loadModel(modelFile)

// Run inference
val outputResult = onnxEngine.runInference(
    session = sessionResult.getOrThrow(),
    inputData = floatArray,
    inputShape = longArrayOf(1, 3, 224, 224)
)
```

### 5. Model Converter (`ModelConverter.kt`)

Model format conversion and optimization pipeline.

**Phase 1 (Current):** Foundation infrastructure
**Phase 2 (Planned):** Full conversion implementations

**Planned Conversions:**
- ONNX â†’ TensorFlow Lite
- PyTorch â†’ ONNX â†’ TensorFlow Lite
- Quantization (FP32 â†’ FP16 â†’ INT8 â†’ INT4)
- Pruning and layer fusion
- Mobile-specific optimizations

**Optimization Presets:**
```kotlin
enum class OptimizationPreset {
    SPEED,      // Maximize FPS - aggressive quantization
    BALANCED,   // Balance speed/accuracy - selective quantization
    QUALITY,    // Preserve accuracy - minimal optimization
    MEMORY      // Minimize RAM - compression + quantization
}
```

## ğŸ”„ Data Flow

### Model Download & Load Flow

```
User Action â†’ downloadModel()
                    â†“
          [Download Manager]
            â†“ HTTP Request
          [OkHttp Client]
            â†“ Stream data
        [Progress Updates] â†’ UI
            â†“
       [Save to Storage]
            â†“
    DownloadState.Completed
            â†“
         loadModel()
            â†“
      [Format Detector]
            â†“
    [Model Converter] (optimize)
            â†“
      [Inference Engine]
            â†“
     [Model Registry]
            â†“
         Ready!
```

### Inference Flow

```
User Input â†’ prepareInput()
                 â†“
         [Model Wrapper]
                 â†“
         Select Engine
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                â–¼
    [ONNX Engine]   [TFLite Engine]
         â”‚                â”‚
         â–¼                â–¼
     run inference    run inference
         â”‚                â”‚
         â–¼                â–¼
    [Process Output]  [Process Output]
         â”‚                â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
            Return Result â†’ UI
```

## ğŸ¨ UI Architecture (Existing)

Neural Forge inherits a sophisticated Jetpack Compose UI from Edge Gallery:

- **HomeScreen**: Model gallery with categories
- **ChatView**: Multi-turn conversation interface
- **ModelPicker**: Model selection and management
- **DownloadPanel**: Progress tracking and controls

## ğŸš€ Performance Optimizations

### Download Performance
- **Chunked Downloads**: 10MB chunks for efficient mobile downloads
- **Resume Support**: Continue interrupted downloads
- **Retry Logic**: Exponential backoff (2s, 4s, 8s, 16s)
- **Connection Pooling**: Reuse connections for efficiency

### Inference Performance
- **Multi-threading**: Use all available CPU cores
- **Graph Optimization**: ONNX graph optimizations
- **Memory Mapping**: Efficient large model loading (planned)
- **Hardware Acceleration**: GPU/NPU/DSP support (planned)

### Memory Management
- **Lazy Loading**: Load models on-demand
- **Resource Cleanup**: Automatic session closure
- **Memory Monitoring**: Track usage and available memory

## ğŸ“Š Device Capabilities

Neural Forge detects and utilizes device hardware:

```kotlin
data class DeviceCapabilities(
    val cpuCores: Int,              // e.g., 8 cores on Snapdragon 8 Gen 2
    val totalMemory: Long,          // Total RAM
    val availableMemory: Long,      // Available RAM
    val supportedAccelerators: Set<Accelerator>
)

enum class Accelerator {
    CPU,      // Always available
    GPU,      // Adreno 740 on S23 Ultra
    NPU,      // Neural Processing Unit
    DSP,      // Hexagon DSP on Snapdragon
    NNAPI     // Android Neural Networks API
}
```

## ğŸ” Security & Privacy

- **Local Processing**: All inference happens on-device
- **No Telemetry**: No model usage tracking (optional analytics)
- **Secure Storage**: Models stored in app-private directory
- **Permission Model**: Minimal permissions required

## ğŸ“± Hardware Optimization (S23 Ultra)

For Snapdragon 8 Gen 2 devices like S23 Ultra:

- **Adreno 740 GPU**: Graphics-optimized operations
- **Hexagon DSP**: Efficient neural network operations
- **Kryo CPU**: 8-core configuration
- **12GB RAM**: Large model support

## ğŸ›£ï¸ Roadmap

### Phase 1 (âœ… Complete)
- [x] Universal format support (TFLite, ONNX, LiteRT-LM)
- [x] Enhanced download manager
- [x] Format detection
- [x] ONNX Runtime integration
- [x] Core architecture

### Phase 2 (In Progress)
- [ ] Model format conversion
- [ ] Advanced UI with Lottie animations
- [ ] GPU acceleration for ONNX
- [ ] Quantization pipeline
- [ ] Split model downloads
- [ ] Model marketplace UI

### Phase 3 (Planned)
- [ ] Model chaining/pipelines
- [ ] P2P model sharing
- [ ] On-device fine-tuning (LoRA)
- [ ] Voice commands
- [ ] Battery-aware scheduling
- [ ] AR model visualization

## ğŸ¤ Contributing

Neural Forge is built on open-source software:

- Based on [Google AI Edge Gallery](https://github.com/google-ai-edge/gallery)
- Uses [ONNX Runtime](https://onnxruntime.ai/)
- Integrates [TensorFlow Lite](https://www.tensorflow.org/lite)

## ğŸ“„ License

Apache License 2.0 - See [LICENSE](LICENSE) for details.

---

**Neural Forge** - Transforming mobile devices into AI powerhouses ğŸ”¥
